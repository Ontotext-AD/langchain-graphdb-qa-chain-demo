{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1271ba5c-1700-4872-b193-f7c162944521",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T18:44:53.493675Z",
     "iopub.status.busy": "2024-03-27T18:44:53.493473Z",
     "iopub.status.idle": "2024-03-27T18:44:53.499541Z",
     "shell.execute_reply": "2024-03-27T18:44:53.498940Z",
     "shell.execute_reply.started": "2024-03-27T18:44:53.493660Z"
    }
   },
   "source": [
    "# Ontotext GraphDB\n",
    "\n",
    ">[Ontotext GraphDB](https://graphdb.ontotext.com/) is a graph database and knowledge discovery tool compliant with [RDF](https://www.w3.org/RDF/) and [SPARQL](https://www.w3.org/TR/sparql11-query/).\n",
    "\n",
    ">This notebook shows how to use LLMs to provide natural language querying (NLQ to SPARQL, also called `text2sparql`) for `Ontotext GraphDB`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a7a98-7d73-4a1a-8860-76a33451d1be",
   "metadata": {
    "id": "922a7a98-7d73-4a1a-8860-76a33451d1be"
   },
   "source": [
    "## GraphDB LLM Functionalities\n",
    "\n",
    "`GraphDB` supports some LLM integration functionalities as described [here](https://github.com/w3c/sparql-dev/issues/193):\n",
    "\n",
    "[gpt-queries](https://graphdb.ontotext.com/documentation/10.7/gpt-queries.html)\n",
    "\n",
    "* magic predicates to ask an LLM for text, list or table using data from your knowledge graph (KG)\n",
    "* query explanation\n",
    "* result explanation, summarization, rephrasing, translation\n",
    "\n",
    "[retrieval-graphdb-connector](https://graphdb.ontotext.com/documentation/10.7/retrieval-graphdb-connector.html)\n",
    "\n",
    "* Indexing of KG entities in a vector database\n",
    "* Supports any text embedding algorithm and vector database\n",
    "* Uses the same powerful connector (indexing) language that GraphDB uses for Elastic, Solr, Lucene\n",
    "* Automatic synchronization of changes in RDF data to the KG entity index\n",
    "* Supports nested objects (no UI support in GraphDB version 10.5)\n",
    "* Serializes KG entities to text like this (e.g. for a Wines dataset):\n",
    "\n",
    "```\n",
    "Franvino:\n",
    "- is a RedWine.\n",
    "- made from grape Merlo.\n",
    "- made from grape Cabernet Franc.\n",
    "- has sugar dry.\n",
    "- has year 2012.\n",
    "```\n",
    "\n",
    "[talk-to-graph](https://graphdb.ontotext.com/documentation/10.7/talk-to-graph.html)\n",
    "\n",
    "* A simple chatbot using a defined KG entity index\n",
    "\n",
    "\n",
    "For this tutorial, we won't use the GraphDB LLM integration, but `SPARQL` generation from NLQ. We'll use the `Star Wars API` (`SWAPI`) ontology and dataset that you can examine [here](https://github.com/Ontotext-AD/langchain-graphdb-qa-chain-demo/blob/main/starwars-data.trig).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b464ff-8556-403f-a3d6-14ffcd703313",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "\n",
    "You need a running GraphDB instance. This tutorial shows how to run the database locally using the [GraphDB Docker image](https://hub.docker.com/r/ontotext/graphdb). It provides a docker compose set-up, which populates GraphDB with the Star Wars dataset. All necessary files including this notebook can be downloaded from the GitHub repository [langchain-graphdb-qa-chain-demo](https://github.com/Ontotext-AD/langchain-graphdb-qa-chain-demo).\n",
    "\n",
    "* Install [Docker](https://docs.docker.com/get-docker/).\n",
    "* Clone the GitHub repository [langchain-graphdb-qa-chain-demo](https://github.com/Ontotext-AD/langchain-graphdb-qa-chain-demo) in a local folder on your machine.\n",
    "* Start GraphDB with the following script executed from the same folder\n",
    "  \n",
    "```\n",
    "docker build --tag graphdb .\n",
    "docker compose up -d graphdb\n",
    "```\n",
    "\n",
    "  You need to wait a couple of seconds for the database to start on `http://localhost:7200/`. The Star Wars dataset `starwars-data.trig` is automatically loaded into the `langchain` repository. The local SPARQL endpoint `http://localhost:7200/repositories/langchain` can be used to run queries against. You can also open the GraphDB Workbench from your favourite web browser `http://localhost:7200/sparql` where you can make queries interactively.\n",
    "* Set up working environment\n",
    "\n",
    "If you use `conda`, create and activate a new conda env (e.g. `conda create -n graph_ontotext_graphdb_qa python=3.12`).\n",
    "\n",
    "Install the following libraries:\n",
    "\n",
    "```\n",
    "pip install jupyter==1.0.0\n",
    "pip install sparqlwrapper==2.0.0\n",
    "pip install rdflib==7.0.0\n",
    "pip install langchain-openai\n",
    "pip install langchain\n",
    "```\n",
    "\n",
    "Run Jupyter with\n",
    "```\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bfb438-98c9-4ebf-bfd5-2e7a40060951",
   "metadata": {},
   "source": [
    "## Connect to GraphDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc8792e0-acfb-4310-b5fa-8f649e448870",
   "metadata": {
    "id": "dc8792e0-acfb-4310-b5fa-8f649e448870"
   },
   "outputs": [],
   "source": [
    "from langchain_community.graphs import OntotextGraphDBGraph\n",
    "\n",
    "graph = OntotextGraphDBGraph(\n",
    "    gdb_repository=\"http://localhost:7200/repositories/langchain\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78548a6-764f-415d-9b9f-ee13920a2760",
   "metadata": {},
   "source": [
    "If you're running a secured GraphDB, you can pass the authentication header in custom_http_headers\n",
    "```\n",
    "graph = OntotextGraphDBGraph(\n",
    "    gdb_repository=...,\n",
    "    custom_http_headers=\"Authorization: <auth-scheme> <authorization-parameters>\",\n",
    ")\n",
    "```\n",
    "\n",
    "Alternativly, if you're using a basic authentication, you can set the environment variables `GRAPHDB_USERNAME` and `GRAPHDB_PASSWORD` before the initialization of `OntotextGraphDBGraph`.\n",
    "```\n",
    "os.environ[\"GRAPHDB_USERNAME\"] = \"graphdb-user\"\n",
    "os.environ[\"GRAPHDB_PASSWORD\"] = \"graphdb-password\"\n",
    "\n",
    "graph = OntotextGraphDBGraph(\n",
    "    gdb_repository=...,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d016b381-d838-4a82-8dfa-5bca6ea03bef",
   "metadata": {},
   "source": [
    "## Setup Ontotext GraphDB QA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab63d88-511d-4049-9bf0-ca8748f1fbff",
   "metadata": {
    "id": "fab63d88-511d-4049-9bf0-ca8748f1fbff"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chains import OntotextGraphDBQAChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# We'll be using an OpenAI model which requires an OpenAI API Key.\n",
    "# However, other models are available as well:\n",
    "# https://python.langchain.com/docs/integrations/chat/\n",
    "\n",
    "# Set the environment variable `OPENAI_API_KEY` to your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-***\"\n",
    "\n",
    "# Any available OpenAI model can be used here.\n",
    "# We use 'gpt-4o' because of the bigger context window.\n",
    "# Check the OpenAI models here https://platform.openai.com/docs/models\n",
    "\n",
    "chain = OntotextGraphDBQAChain.from_llm(\n",
    "    ChatOpenAI(\n",
    "        temperature=0,\n",
    "        model_name=\"gpt-4o-2024-05-13\",\n",
    "        seed=123,\n",
    "    ),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51b397c-2fdc-4b99-9fed-1ab2b6ef7547",
   "metadata": {
    "id": "e51b397c-2fdc-4b99-9fed-1ab2b6ef7547"
   },
   "source": [
    "## Specifying the ontology\n",
    "\n",
    "In order for the LLM to be able to generate SPARQL, it needs to know the knowledge graph schema (the ontology). The ontology schema dump should:\n",
    "\n",
    "* Include enough information about classes, properties, property attachment to classes (using rdfs:domain, schema:domainIncludes or OWL restrictions), and taxonomies (important individuals).\n",
    "* Not include overly verbose and irrelevant definitions and examples that do not help SPARQL construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c99000-a4e9-48d1-aa7d-f9d429e32bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ontology_schema = Path(\n",
    "    \"/path/to/langchain-graphdb-qa-chain-demo/starwars-ontology.ttl\"\n",
    ").read_text(encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d8a00-c98f-43b8-9e84-77b244f7bb24",
   "metadata": {
    "id": "446d8a00-c98f-43b8-9e84-77b244f7bb24"
   },
   "source": [
    "## Question Answering against the StarWars dataset\n",
    "\n",
    "We can now use the `OntotextGraphDBQAChain` to ask some questions. Let's ask a simple one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1dc4bea-b0f1-48f7-99a6-351a31acac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new OntotextGraphDBQAChain chain...\u001b[0m\n",
      "Generated query:\n",
      "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
      "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
      "\n",
      "SELECT ?climate\n",
      "WHERE {\n",
      "  ?planet rdfs:label \"Tatooine\" .\n",
      "  ?planet <https://swapi.co/vocabulary/climate> ?climate .\n",
      "}\n",
      "LIMIT 5\n",
      "\u001b[32;1m\u001b[1;3mQuery results:\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m[{\"climate\": \"arid\"}]\u001b[0m\n",
      "Finished chain for 3.26 seconds\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The climate on Tatooine is arid.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"What is the climate on Tatooine?\",\n",
    "        \"ontology_schema\": ontology_schema,\n",
    "    }\n",
    ")[chain.output_key_answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3ce3e-9528-4a65-8f3e-2281de08cbf1",
   "metadata": {},
   "source": [
    "We can also ask more complicated questions like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab6f55f1-a3e0-4615-abd2-3cb26619c8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new OntotextGraphDBQAChain chain...\u001b[0m\n",
      "Generated query:\n",
      "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
      "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
      "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
      "PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
      "\n",
      "SELECT (AVG(?boxOffice) AS ?averageBoxOffice)\n",
      "WHERE {\n",
      "  ?film rdf:type <https://swapi.co/vocabulary/Film> ;\n",
      "        <https://swapi.co/vocabulary/boxOffice> ?boxOffice .\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3mQuery results:\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m[{\"averageBoxOffice\": \"754147643.8\"}]\u001b[0m\n",
      "Finished chain for 8.08 seconds\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The average box office revenue for all the Star Wars movies is $754,147,643.80.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"What is the average box office revenue for all the Star Wars movies?\",\n",
    "        \"ontology_schema\": ontology_schema,\n",
    "    }\n",
    ")[chain.output_key_answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11511345-8436-4634-92c6-36f2c0dd44db",
   "metadata": {
    "id": "11511345-8436-4634-92c6-36f2c0dd44db"
   },
   "source": [
    "## Chain prompts\n",
    "\n",
    "The Ontotext GraphDB QA chain allows prompt refinement for further improvement of your QA chain and enhancing the overall user experience of your app.\n",
    "\n",
    "\n",
    "### \"SPARQL Generation\" prompt\n",
    "\n",
    "The prompt is used for the SPARQL query generation based on the user question and the KG schema.\n",
    "\n",
    "- `sparql_generation_prompt`\n",
    "\n",
    "    Default value:\n",
    "  ````python\n",
    "    GRAPHDB_SPARQL_GENERATION_TEMPLATE = \"\"\"\n",
    "    Write a SPARQL SELECT query to answer the user question delimited by triple backticks:\\n```{question}```\\n\n",
    "    The ontology schema delimited by triple backticks in Turtle format is:\\n```{ontology_schema}```\\n\n",
    "    Use only the classes and properties provided in the schema to construct the SPARQL query. \n",
    "    Do not use any classes or properties that are not explicitly provided in the SPARQL query. \n",
    "    Include all necessary prefixes. \n",
    "    Do not include any explanations or apologies in your responses. \n",
    "    Do not wrap the query in backticks. \n",
    "    Do not include any text except the SPARQL query generated. \n",
    "    For queries without aggregation, apply LIMIT 5 unless otherwise specified. \n",
    "    For queries with aggregation, don't apply limit unless otherwise specified. \\n\n",
    "    \"\"\"\n",
    "    GRAPHDB_SPARQL_GENERATION_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"ontology_schema\", \"question\"],\n",
    "        template=GRAPHDB_SPARQL_GENERATION_TEMPLATE,\n",
    "    )\n",
    "  ````\n",
    "\n",
    "Note that if you change the default value of the prompt, you must call the chain by providing values for each of the input variables of the prompt.\n",
    "\n",
    "### \"SPARQL Fix\" prompt\n",
    "\n",
    "Sometimes, the LLM may generate a SPARQL query with syntactic errors or missing prefixes, etc. The chain will try to amend this by prompting the LLM to correct it a certain number of times.\n",
    "\n",
    "- `sparql_fix_prompt`\n",
    "\n",
    "    Default value:\n",
    "  ````python\n",
    "    GRAPHDB_SPARQL_FIX_TEMPLATE = \"\"\"\n",
    "    The following SPARQL query delimited by triple backticks\n",
    "    ```\n",
    "    {generated_sparql}\n",
    "    ```\n",
    "    is not valid.\n",
    "    The error delimited by triple backticks is\n",
    "    ```\n",
    "    {error_message}\n",
    "    ```\n",
    "    Give me a correct version of the SPARQL query.\n",
    "    Do not change the logic of the query.\n",
    "    Do not include any explanations or apologies in your responses.\n",
    "    Do not wrap the query in backticks.\n",
    "    Do not include any text except the SPARQL query generated.\n",
    "    The ontology schema delimited by triple backticks in Turtle format is:\n",
    "    ```\n",
    "    {ontology_schema}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    GRAPHDB_SPARQL_FIX_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"error_message\", \"generated_sparql\", \"ontology_schema\"],\n",
    "        template=GRAPHDB_SPARQL_FIX_TEMPLATE,\n",
    "    )\n",
    "  ````\n",
    "\n",
    "Note that the input values passed to the prompt include the error message `error_message`, the generated SPARQL query `generated_sparql`, and the input values passed to the chain. If you want to include extra variables, you must pass them as input to the chain.\n",
    "\n",
    "- `max_fix_retries`\n",
    "  \n",
    "    Default value: `5`\n",
    "\n",
    "### \"Answering\" prompt\n",
    "\n",
    "The prompt is used for answering the question based on the results returned from the database and the initial user question. By default, the LLM is instructed to only use the information from the returned result(s). If the result set is empty, the LLM should inform that it can't answer the question.\n",
    "\n",
    "- `qa_prompt`\n",
    "  \n",
    "  Default value:\n",
    "  ````python\n",
    "    GRAPHDB_QA_TEMPLATE = \"\"\"Task: Generate a natural language response from the results of a SPARQL query.\n",
    "    You are an assistant that creates well-written and human understandable answers.\n",
    "    The information part contains the information provided, which you can use to construct an answer.\n",
    "    The information provided is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\n",
    "    Make your response sound like the information is coming from an AI assistant, but don't add any information.\n",
    "    Don't use internal knowledge to answer the question, just say you don't know if no information is available.\n",
    "    Information:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "    GRAPHDB_QA_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"], template=GRAPHDB_QA_TEMPLATE\n",
    "    )\n",
    "  ````\n",
    "\n",
    "Note that the input values passed to the prompt include the SPARQL query results `context`, and the input values passed to the chain. If you want to include extra variables, you must pass them as input to the chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b09696b-43cb-47b3-b4b8-d947731d9592",
   "metadata": {},
   "source": [
    "## Modifying \"SPARQL Generation\" prompt example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e58d72e6-3046-4482-85a7-10149792f698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Luke Skywalker's home planet is Tatooine.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Write a SPARQL SELECT query to answer the user question \n",
    "delimited by triple backticks:\\n```{question}```\\n\n",
    "The question mentions the following concepts in JSON format \n",
    "delimited by triple backticks\\n```{named_entities}```\\n\n",
    "The ontology schema delimited by triple backticks in \n",
    "Turtle format is:\\n```{ontology_schema}```\\n\n",
    "Use only the classes and properties provided in the schema \n",
    "to construct the SPARQL query.\n",
    "Do not use any classes or properties \n",
    "that are not explicitly provided in the SPARQL query.\n",
    "Include all necessary prefixes.\n",
    "Do not include any explanations or apologies in your responses.\n",
    "Do not wrap the query in backticks.\n",
    "Do not include any text except the SPARQL query generated.\n",
    "\"\"\"\n",
    "chain = OntotextGraphDBQAChain.from_llm(\n",
    "    ChatOpenAI(\n",
    "        temperature=0,\n",
    "        model_name=\"gpt-4o-2024-05-13\",\n",
    "        seed=123,\n",
    "    ),\n",
    "    graph=graph,\n",
    "    sparql_generation_prompt=PromptTemplate(\n",
    "        input_variables=[\"question\", \"named_entities\", \"ontology_schema\"],\n",
    "        template=template,\n",
    "    ),\n",
    ")\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"question\": \"What is the name of Luke Skywalker's home planet?\",\n",
    "        \"ontology_schema\": ontology_schema,\n",
    "        \"named_entities\": [\n",
    "            {\n",
    "                \"class\": \"https://swapi.co/vocabulary/Human\",\n",
    "                \"inst\": \"https://swapi.co/resource/human/1\",\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    ")[chain.output_key_answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef8c073-003d-44ab-8a7b-cf45c50f6370",
   "metadata": {
    "id": "2ef8c073-003d-44ab-8a7b-cf45c50f6370"
   },
   "source": [
    "Once you're finished playing with QA with GraphDB, you can shut down the Docker environment by running\n",
    "``\n",
    "docker compose down -v --remove-orphans\n",
    "``\n",
    "from the directory with the Docker compose file."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
